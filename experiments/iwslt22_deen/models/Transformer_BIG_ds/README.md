# nmt-scripts

This repository contains scripts and templates for training NMT models with the fairseq toolkit.

The goal is to automate as much as possible the development of new NMT systems, in order to:

1. Automatically carry out repetitive parts of the pipeline that do not change across systems,
   significantly reducing the user time it takes to develop a new system.

2. Reduce the chance of user error, by means of a single config file that is then used across the pipeline.

## Usage

The philosophy behind nmt-scripts is very simple: *1 folder, 1 system*. That is, it is expected that you clone this repository as many times as systems you want to train, with each folder
containing all the configuration for its associated system. Trying to launch multiple systems from one folder will have unintended consequences. It is better to first prepare the config
for one system, and then copy that folder in order to re-use the config.

The following steps allow you to run the whole pipeline:

1. Clone this repo into a new folder.
2. Copy/link the train/dev/test raw data into a folder of your choice (remember to point `$CORPUS_FOLDER` to this folder in the next step).
3. Edit the config.sh file with the appropiate values.
4. Run the pipeline by launching the `run_experiment` script.

```
./run_experiment_bpe_FP16.sh
```

*The run_experiment script should be launched from the same directory where it is saved*. This script applies preprocessing and BPE to your data, trains a Transformer BASE system, and carries out inference on your dev and test data.

If you only wish to carry out certain parts of the whole pipeline, that can be done by calling the individual scripts:
* `preprocess_corpus_only_clean_tokenize_truecase.sh`: Preprocess the corpus (tokenization, cleaning and truecasing).
* `learn_and_apply_bpe.sh`: Apply BPE subword segmentation to train, dev and test data.
* `prepare_data_fairseq.sh`: Binarize training data in the format used by the specific toolkit.
* `train_model_fairseq_BASE_FP16.sh`: Train a Transforer BASE system.
* `average_checkpoints_and_overwrite_best.sh`: Applies checkpoint averaging, which is an effective technique for Transformer models.
* `infer_fairseq.sh`: Carry out inference on the dev/test data.

## The config.sh file

The main logic is held in the config.sh file. This file holds all the important information required
for system training. Once the data has been obtained, the idea is to be able to carry out
the entire pipeline only by setting the appropiate variables inside config.sh and selecting
a training recipe. Unless the programmer wants some very specific behaviour, there is no need to edit any other file apart from config.sh. 

When conducting a new experiment, one must therefore provide the data to be used, and set the following fields of the config file (These mandatory fields are annotated with a `#Fill the following fields` comment):

* `CORPUS`: Name that describes the corpus/task that will be used for training. This identifier will be used later for logging and storing artefacts.
* `RUN`: An identifier for the model to be trained. Usually one can expect to train multiple runs for each corpus. By default, the name is taken from the folder name.
* `CORPUS_FOLDER`: Folder that holds the data files.   
* `SOURCE/TARGET_LANG_SUFFIX`: Language suffix of your data files (ISO 639â€‘1 Code, for example, `'es'` for Spanish)  
* `ORIG_{TRAIN/DEV/TEST}_PREFIX`: Data files prefix. These are the names of the files holding your corpus. Thus, for example, 
if your TRAIN/DEV/TEST prefixes are corpus/dev/test, and you are training a Spanish-English system, your `$CORPUS_FOLDER` should contain:  

```
corpus.es
corpus.en
dev.es
dev.en
test.es
test.en
```

Aport from the previously mentioned ones, it is also interesting to know about:

* `MODEL_OUTPUT_FOLDER` and `MODEL_INFER_FOLDER`: The folders used to store the checkpoints and inference, respectively. By default, they make use of $CORPUS and $RUN. Thus, you should be careful
to give different values to $RUN for each experiment, otherwise your results might be overwritten. If you use the default values, make sure that the `/scratch/"$USER"/nmt-scripts-output/experiments/mt/`
folder exists.
* `MODEL_CONFIG`: Name of the model config that will be used to train the model. The toolkit will use the file `./train_model_"$TOOLKIT"_"$MODEL_CONFIG".sh` to train the model.

## Evaluation
Since evaluation is the most critical step, it is recommended that the user configure the evaluation for each specific use case, ideally using a tool such as sacrebleu. 
The `compute_bleu_detokedREF.sh` script may be used to evaluate against the provided test set using sacrebleu, but it is better to use hardcoded sacrebleu test sets if these are available.

## Recipes
Up to this point, only the default pipeline has been explained. It is expected that one would want to improve the quality of the results after having learned how to use the
toolkit. 

If enough training data is available (10Ms of sentence pairs), one of the first changes one would probably want to do is to train a Trasnformer model using
the BIG configuration instead of the BASE one. This can be done by copying the `train_model_scripts/train_model_fairseq_BIG_FP16.sh` recipe into the root folder, and setting
`MODEL_CONFIG=BIG` and `GPU_MEM=10.5G` inside config.sh.

The `preprocess_corpus_scripts`, `run_experiment_scripts` and `train_model_scripts` contain many recipes that might be useful to you at some point.

There exists a recipe that uses SentencePiece(spm) subword segmentation instead of BPE (`run_experiment_spm_FP16.sh` and so on) . However, it is unclear if this offers quality advantages, and currently it is much harder to interpret because it prints subword indexes instead of subwords. Thus, it is reccomended to stick with the BPE recipe unless working in a simultaneous/online scenario.

Other scripts of interest are contained in the following folders:
* `filtering/`: Setups for different data cleaning/filtering configurations. This is almost mandatory if you have noisy data.
* `finetuning/`: An example of finetuning/domain adaptation. Very useful if you have in-domain data.
* `production/`: Automate deployment of production-ready systems. Some steps need to be carried out manually.

### A word of caution
All recipes committed to the repo have been tested on multiple systems. However, due to continuous improvements, there might be some that are not backward/forward compatible,
or rely on some hardcoded artefacts to which you don't have access.
